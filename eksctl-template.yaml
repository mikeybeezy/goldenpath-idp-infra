# eksctl cluster config template
# ------------------------------------
# This template defines a basic EKS cluster using eksctl.
# Commented instructions explain each sectionâ€”uncomment and modify
# as needed before running `eksctl create cluster -f eksctl-template.yaml`.

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: devops-test-cluster-c          # <--- replace with the cluster name (e.g., goldenpath-dev)
  region: eu-west-2                # <--- AWS region for the cluster (e.g., eu-west-2)
  version: "1.29"                   # <--- Kubernetes version; check eksctl docs for supported releases

# vpc:
#   cidr: "10.0.0.0/16"               # <--- optional: specify VPC CIDR if you want eksctl to create the VPC
#   # subnets:                        # <--- optional: define custom subnets if you have existing networking
  #   private:
  #     us-east-1a: { id: subnet-xxxx }
  #     us-east-1b: { id: subnet-yyyy }
  #   public:
  #     us-east-1a: { id: subnet-zzzz }

iam:
  withOIDC: true                    # <--- enable OIDC for IAM roles for service accounts
  serviceAccounts: []               # <--- optionally define IRSA bindings here

# cloudWatch:
#   clusterLogging:
#     enableTypes: ["audit", "authenticator"]  # <--- choose log types to ship to CloudWatch

# Node group definition: you can add multiple managed node groups.
managedNodeGroups:
  - name: devops-test-ng-c
    instanceType: t3.micro         # <--- EC2 instance type for worker nodes
    desiredCapacity: 2              # <--- number of nodes to run by default
    minSize: 1
    maxSize: 4
    ssh:
      enableSsm: true               # <--- optional: allow SSM to access nodes (recommended over SSH keys)
    tags:
      Environment: dev              # <--- optional: tags applied to node group resources
  - name: devops-test-ng-d
    instanceType: t3.small         # <--- EC2 instance type for worker nodes
    desiredCapacity: 2              # <--- number of nodes to run by default
    minSize: 1
    maxSize: 4
    ssh:
      enableSsm: true               # <--- optional: allow SSM to access nodes (recommended over SSH keys)
    tags:
      Environment: dev              # <--- optional: tags applied to node group resources

# addons section is optional; include if you need AWS-managed addons (e.g., VPC CNI, CoreDNS overrides).
# addons:
#   - name: vpc-cni
#     version: latest
#   - name: coredns
#     version: latest

# If you already have IAM roles, security groups, or cluster-level policies to attach, document them here.
# For more advanced settings (Bottlerocket, self-managed nodegroups, Fargate profiles), refer to eksctl docs.



# You can update the node group without tearing
# down the cluster. With eksctl: Modify your 
# cluster config YAML to add a
# new node group or change instance type 
# (e.g., add managedNodeGroups entry with 
# instanceType: t3.small).




# 2.
#You can update the node group without tearing down the cluster. With eksctl:
#Modify your cluster config YAML to add a new node group or change instance 
#type (e.g., add managedNodeGroups entry with instanceType: t3.small).

# eksctl create nodegroup -f <config.yaml>


# This adds the new node group alongside the existing one.

# 3 To change the current nodes, either: Scale the old node group down and remove it:

# eksctl delete nodegroup --cluster <name> --name <old-ng>

# (after workloads drain/move to the new group), or
# Use eksctl scale nodegroup --cluster <name> --name <ng> --nodes <count>
# to adjust size without deleting.