# Owner: platform
name: Ops - CI Teardown

on:
  workflow_dispatch:
    inputs:
      env:
        description: "Environment to tear down (dev/test/staging/prod)"
        required: true
        default: "dev"
        type: choice
        options:
          - dev
          - test
          - staging
          - prod
      region:
        description: "AWS region"
        required: true
        default: "eu-west-2"
      cluster_name:
        description: "EKS cluster name (optional, resolved if empty)"
        required: false
        default: ""
      build_id:
        description: "Build ID for teardown"
        required: true
        default: ""
      lifecycle:
        description: "Lifecycle (ephemeral or persistent)"
        required: true
        default: "ephemeral"
        type: choice
        options:
          - ephemeral
          - persistent
      teardown_version:
        description: "Teardown script version"
        required: true
        default: "v1"
        type: choice
        options:
          - v1
          - v2
      relax_pdb:
        description: "Relax PDB during teardown"
        required: true
        default: "true"
      drain_timeout:
        description: "Node drain timeout"
        required: true
        default: "300s"
      heartbeat_interval:
        description: "Teardown heartbeat interval (seconds)"
        required: true
        default: "30"
      cleanup_mode:
        description: "Orphan cleanup mode (delete, dry_run, none)"
        required: true
        default: "delete"
        type: choice
        options:
          - delete
          - dry_run
          - none
      argo_app_namespace:
        description: "Argo CD application namespace to delete before LB cleanup"
        required: true
        default: "kong-system"
      argo_app_name:
        description: "Argo CD application name to delete before LB cleanup"
        required: true
        default: "dev-kong"
      delete_argo_app:
        description: "Delete Argo CD application before LB cleanup"
        required: true
        default: true
        type: boolean
      lb_cleanup_max_wait:
        description: "Max seconds to wait for LoadBalancer services to disappear"
        required: true
        default: "900"
      force_delete_lbs:
        description: "Delete remaining Kubernetes load balancers if ENIs persist"
        required: true
        default: true
        type: boolean
      force_delete_lb_finalizers:
        description: "Remove stuck LoadBalancer service finalizers"
        required: true
        default: true
        type: boolean

jobs:
  teardown:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    env:
      ENV: ${{ inputs.env }}
      AWS_REGION: ${{ inputs.region }}
      CLUSTER_NAME: ${{ inputs.cluster_name }}
      BUILD_ID: ${{ inputs.build_id }}
      CLUSTER_LIFECYCLE: ${{ inputs.lifecycle }}
      TEARDOWN_VERSION: ${{ inputs.teardown_version }}
      TF_DIR: envs/${{ inputs.env }}
      CLEANUP_ORPHANS: ${{ inputs.cleanup_mode != 'none' }}
      ORPHAN_CLEANUP_MODE: ${{ inputs.cleanup_mode }}
      ARGO_APP_NAMESPACE: ${{ inputs.argo_app_namespace }}
      ARGO_APP_NAME: ${{ inputs.argo_app_name }}
      DELETE_ARGO_APP: ${{ inputs.delete_argo_app }}
      LB_CLEANUP_MAX_WAIT: ${{ inputs.lb_cleanup_max_wait }}
      FORCE_DELETE_LBS: ${{ inputs.force_delete_lbs }}
      FORCE_DELETE_LB_FINALIZERS: ${{ inputs.force_delete_lb_finalizers }}
      REMOVE_K8S_SA_FROM_STATE: "true"
      TF_AUTO_APPROVE: "true"
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
      - name: Validate branch
        run: |
          if [[ "${{ github.ref_name }}" != "main" && "${{ github.ref_name }}" != "development" ]]; then
            echo "Teardown can only run from main or development." >&2
            exit 1
          fi

      - name: Resolve cluster name
        run: |
          if [[ -z "${CLUSTER_NAME}" ]]; then
            CLUSTER_NAME="$(ENV=${ENV} TF_DIR=${TF_DIR} BUILD_ID=${BUILD_ID} CLUSTER_LIFECYCLE=${CLUSTER_LIFECYCLE} bash scripts/resolve-cluster-name.sh)"
          fi
          echo "CLUSTER_NAME=${CLUSTER_NAME}" >> "${GITHUB_ENV}"
          echo "Resolved CLUSTER_NAME=${CLUSTER_NAME}"

      - name: Resolve backend config
        id: backend
        run: |
          case "${ENV}" in
            dev)
              echo "role=${{ secrets.TF_AWS_IAM_ROLE_DEV_APPLY }}" >> "$GITHUB_OUTPUT"
              echo "bucket=goldenpath-idp-dev-bucket" >> "$GITHUB_OUTPUT"
              echo "table=goldenpath-idp-dev-db-key" >> "$GITHUB_OUTPUT"
              ;;
            test)
              echo "role=${{ secrets.TF_AWS_IAM_ROLE_TEST }}" >> "$GITHUB_OUTPUT"
              echo "bucket=goldenpath-test-staging-bucket" >> "$GITHUB_OUTPUT"
              echo "table=goldenpath-idp-test-db" >> "$GITHUB_OUTPUT"
              ;;
            staging)
              echo "role=${{ secrets.TF_AWS_IAM_ROLE_STAGING }}" >> "$GITHUB_OUTPUT"
              echo "bucket=goldenpath-idp-staging-bucket" >> "$GITHUB_OUTPUT"
              echo "table=goldenpath-idp-staging-db" >> "$GITHUB_OUTPUT"
              ;;
            prod)
              echo "role=${{ secrets.TF_AWS_IAM_ROLE_PROD }}" >> "$GITHUB_OUTPUT"
              echo "bucket=goldenpath-idp-prod-bucket" >> "$GITHUB_OUTPUT"
              echo "table=goldenpath-idp-prod-db" >> "$GITHUB_OUTPUT"
              ;;
            *)
              echo "Unsupported env: ${ENV}" >&2
              exit 1
              ;;
          esac

      - name: Resolve state key
        run: |
          if [[ "${CLUSTER_LIFECYCLE}" == "ephemeral" ]]; then
            STATE_KEY="envs/${ENV}/builds/${BUILD_ID}/terraform.tfstate"
          else
            STATE_KEY="envs/${ENV}/terraform.tfstate"
          fi
          echo "STATE_KEY=${STATE_KEY}" >> "${GITHUB_ENV}"
          echo "Resolved STATE_KEY=${STATE_KEY}"

      - name: Configure AWS credentials using OIDC
        uses: aws-actions/configure-aws-credentials@v4.1.0
        with:
          aws-region: ${{ inputs.region }}
          role-to-assume: ${{ steps.backend.outputs.role }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform init (backend)
        run: |
          terraform -chdir="${TF_DIR}" init -reconfigure \
            -backend-config="bucket=${{ steps.backend.outputs.bucket }}" \
            -backend-config="key=${STATE_KEY}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${{ steps.backend.outputs.table }}" \
            -backend-config="encrypt=true"

      - name: Run timed teardown
        run: |
          TEARDOWN_CONFIRM="true" \
          RELAX_PDB="${{ inputs.relax_pdb }}" \
          DRAIN_TIMEOUT="${{ inputs.drain_timeout }}" \
          HEARTBEAT_INTERVAL="${{ inputs.heartbeat_interval }}" \
          TF_DIR="${TF_DIR}" \
          CLEANUP_ORPHANS="${CLEANUP_ORPHANS}" \
          CLUSTER="${CLUSTER_NAME}" \
          REGION="${AWS_REGION}" \
          make timed-teardown ENV="${ENV}" BUILD_ID="${BUILD_ID}"

      - name: Reliability metrics summary (timed teardown)
        if: ${{ always() }}
        run: |
          make reliability-metrics
      - name: Runbook link
        if: ${{ always() }}
        run: |
          echo "Runbook: docs/runbooks/09_CI_TEARDOWN_RECOVERY_V2.md" >> "${GITHUB_STEP_SUMMARY}"

  teardown-resume:
    runs-on: ubuntu-latest
    needs: teardown
    if: ${{ always() && needs.teardown.result == 'failure' }}
    permissions:
      contents: read
      id-token: write
    env:
      ENV: ${{ inputs.env }}
      AWS_REGION: ${{ inputs.region }}
      CLUSTER_NAME: ${{ inputs.cluster_name }}
      BUILD_ID: ${{ inputs.build_id }}
      CLUSTER_LIFECYCLE: ${{ inputs.lifecycle }}
      TEARDOWN_VERSION: ${{ inputs.teardown_version }}
      TF_DIR: envs/${{ inputs.env }}
      CLEANUP_ORPHANS: ${{ inputs.cleanup_mode != 'none' }}
      ORPHAN_CLEANUP_MODE: ${{ inputs.cleanup_mode }}
      ARGO_APP_NAMESPACE: ${{ inputs.argo_app_namespace }}
      ARGO_APP_NAME: ${{ inputs.argo_app_name }}
      DELETE_ARGO_APP: ${{ inputs.delete_argo_app }}
      LB_CLEANUP_MAX_WAIT: ${{ inputs.lb_cleanup_max_wait }}
      FORCE_DELETE_LBS: ${{ inputs.force_delete_lbs }}
      FORCE_DELETE_LB_FINALIZERS: ${{ inputs.force_delete_lb_finalizers }}
      REMOVE_K8S_SA_FROM_STATE: "true"
      TF_AUTO_APPROVE: "true"
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Resolve cluster name
        run: |
          if [[ -z "${CLUSTER_NAME}" ]]; then
            CLUSTER_NAME="$(ENV=${ENV} TF_DIR=${TF_DIR} BUILD_ID=${BUILD_ID} CLUSTER_LIFECYCLE=${CLUSTER_LIFECYCLE} bash scripts/resolve-cluster-name.sh)"
          fi
          echo "CLUSTER_NAME=${CLUSTER_NAME}" >> "${GITHUB_ENV}"
          echo "Resolved CLUSTER_NAME=${CLUSTER_NAME}"

      - name: Resolve backend config
        id: backend
        run: |
          case "${ENV}" in
            dev)
              echo "role=${{ secrets.TF_AWS_IAM_ROLE_DEV_APPLY }}" >> "$GITHUB_OUTPUT"
              echo "bucket=goldenpath-idp-dev-bucket" >> "$GITHUB_OUTPUT"
              echo "table=goldenpath-idp-dev-db-key" >> "$GITHUB_OUTPUT"
              ;;
            test)
              echo "role=${{ secrets.TF_AWS_IAM_ROLE_TEST }}" >> "$GITHUB_OUTPUT"
              echo "bucket=goldenpath-test-staging-bucket" >> "$GITHUB_OUTPUT"
              echo "table=goldenpath-idp-test-db" >> "$GITHUB_OUTPUT"
              ;;
            staging)
              echo "role=${{ secrets.TF_AWS_IAM_ROLE_STAGING }}" >> "$GITHUB_OUTPUT"
              echo "bucket=goldenpath-idp-staging-bucket" >> "$GITHUB_OUTPUT"
              echo "table=goldenpath-idp-staging-db" >> "$GITHUB_OUTPUT"
              ;;
            prod)
              echo "role=${{ secrets.TF_AWS_IAM_ROLE_PROD }}" >> "$GITHUB_OUTPUT"
              echo "bucket=goldenpath-idp-prod-bucket" >> "$GITHUB_OUTPUT"
              echo "table=goldenpath-idp-prod-db" >> "$GITHUB_OUTPUT"
              ;;
            *)
              echo "Unsupported env: ${ENV}" >&2
              exit 1
              ;;
          esac

      - name: Resolve state key
        run: |
          if [[ "${CLUSTER_LIFECYCLE}" == "ephemeral" ]]; then
            STATE_KEY="envs/${ENV}/builds/${BUILD_ID}/terraform.tfstate"
          else
            STATE_KEY="envs/${ENV}/terraform.tfstate"
          fi
          echo "STATE_KEY=${STATE_KEY}" >> "${GITHUB_ENV}"
          echo "Resolved STATE_KEY=${STATE_KEY}"

      - name: Configure AWS credentials using OIDC
        uses: aws-actions/configure-aws-credentials@v4.1.0
        with:
          aws-region: ${{ inputs.region }}
          role-to-assume: ${{ steps.backend.outputs.role }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform init (backend)
        run: |
          terraform -chdir="${TF_DIR}" init -reconfigure \
            -backend-config="bucket=${{ steps.backend.outputs.bucket }}" \
            -backend-config="key=${STATE_KEY}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${{ steps.backend.outputs.table }}" \
            -backend-config="encrypt=true"

      - name: Run teardown resume
        run: |
          TEARDOWN_CONFIRM="true" \
          RELAX_PDB="${{ inputs.relax_pdb }}" \
          DRAIN_TIMEOUT="${{ inputs.drain_timeout }}" \
          HEARTBEAT_INTERVAL="${{ inputs.heartbeat_interval }}" \
          TF_DIR="${TF_DIR}" \
          CLEANUP_ORPHANS="${CLEANUP_ORPHANS}" \
          CLUSTER="${CLUSTER_NAME}" \
          REGION="${AWS_REGION}" \
          make teardown-resume ENV="${ENV}" BUILD_ID="${BUILD_ID}"
      - name: Runbook link
        if: ${{ always() }}
        run: |
          echo "Runbook: docs/runbooks/09_CI_TEARDOWN_RECOVERY_V2.md" >> "${GITHUB_STEP_SUMMARY}"
