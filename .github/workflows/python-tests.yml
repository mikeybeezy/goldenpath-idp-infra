# Owner: platform-team
# Reference: ADR-0146 Schema-Driven Script Certification
# Purpose: Run Python tests and generate certification proofs
#
# This workflow:
#   1. Runs pytest with junit.xml output
#   2. Generates proof artifacts linking test results to script IDs
#   3. Uploads proofs for use by validate_scripts_tested.py --verify-proofs
#
# NOTE: Tests are BLOCKING per ADR-0164 Agent Trust Architecture

name: Python Tests & Certification Proofs

on:
  pull_request:
    branches:
      - main
      - development
    paths:
      - 'scripts/**/*.py'
      - 'tests/**/*.py'
      - '.github/workflows/python-tests.yml'
  push:
    branches:
      - main
      - development
    paths:
      - 'scripts/**/*.py'
      - 'tests/**/*.py'

permissions:
  contents: read

jobs:
  unit-tests:
    name: Run Tests & Generate Proofs
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          # Use lightweight CI deps (no ML/RAG packages)
          # RAG tests skip gracefully when deps missing
          pip install -r requirements-ci.txt

      - name: Run Tests with Coverage
        id: tests
        run: |
          mkdir -p test-results coverage

          # Run pytest with junit output and coverage enforcement
          # Current baseline: 20% (TDD bootstrap phase)
          # V1 target: 60%, V1.1 target: 70%
          # Per TDD adoption plan Phase 2
          pytest tests/ \
            --junitxml=test-results/junit.xml \
            --cov=scripts \
            --cov-report=xml:coverage/coverage.xml \
            --cov-report=html:coverage/html \
            --cov-report=term-missing \
            --cov-fail-under=20 \
            --tb=short \
            -v \
            2>&1 | tee test-results/pytest-output.log

          # Capture exit code
          echo "exit_code=${PIPESTATUS[0]}" >> $GITHUB_OUTPUT

      - name: Collect Test Metrics
        if: always()
        run: |
          if [ -f test-results/junit.xml ]; then
            ARGS=(--repo "${{ github.repository }}" \
                  --branch "${{ github.ref_name }}" \
                  --commit "${{ github.sha }}" \
                  --ci-run-id "${{ github.run_id }}" \
                  --pytest-junit test-results/junit.xml)

            if [ -f coverage/coverage.xml ]; then
              ARGS+=(--pytest-coverage-xml coverage/coverage.xml --pytest-threshold 20)
            fi

            python3 scripts/collect_test_metrics.py "${ARGS[@]}" \
              --output test-results/test-metrics.json || \
              echo "Warning: failed to collect test metrics"
          else
            echo "Warning: junit.xml not found; skipping test metrics collection"
          fi

      - name: Upload Coverage Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: coverage/
          retention-days: 30

      - name: Generate Test Proofs
        if: always()
        run: |
          python3 scripts/generate_test_proofs.py \
            --junit-path test-results/junit.xml \
            --output-dir test-results/proofs \
            --commit-sha ${{ github.sha }} \
            --run-id ${{ github.run_id }}

      - name: Generate Test Summary
        if: always()
        run: |
          echo "## Python Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f test-results/junit.xml ]; then
            TESTS=$(grep -o 'tests="[0-9]*"' test-results/junit.xml | head -1 | grep -o '[0-9]*' || echo "0")
            FAILURES=$(grep -o 'failures="[0-9]*"' test-results/junit.xml | head -1 | grep -o '[0-9]*' || echo "0")
            ERRORS=$(grep -o 'errors="[0-9]*"' test-results/junit.xml | head -1 | grep -o '[0-9]*' || echo "0")
            SKIPPED=$(grep -o 'skipped="[0-9]*"' test-results/junit.xml | head -1 | grep -o '[0-9]*' || echo "0")

            echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Total Tests | ${TESTS} |" >> $GITHUB_STEP_SUMMARY
            echo "| Passed | $((TESTS - FAILURES - ERRORS - SKIPPED)) |" >> $GITHUB_STEP_SUMMARY
            echo "| Failed | ${FAILURES} |" >> $GITHUB_STEP_SUMMARY
            echo "| Errors | ${ERRORS} |" >> $GITHUB_STEP_SUMMARY
            echo "| Skipped | ${SKIPPED} |" >> $GITHUB_STEP_SUMMARY
          else
            echo ":warning: No test results found" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Certification Proofs Generated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -d test-results/proofs ]; then
            PROOF_COUNT=$(ls -1 test-results/proofs/*.json 2>/dev/null | wc -l)
            echo "Generated **${PROOF_COUNT}** proof artifacts." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # List proofs with verdicts
            for proof in test-results/proofs/*.json; do
              if [ -f "$proof" ]; then
                SCRIPT_ID=$(jq -r '.script_id' "$proof")
                VERDICT=$(jq -r '.verdict' "$proof")
                PASS_RATE=$(jq -r '.test_summary.pass_rate' "$proof")
                if [ "$VERDICT" = "PASS" ]; then
                  echo "- :white_check_mark: \`${SCRIPT_ID}\`: ${PASS_RATE}% pass rate" >> $GITHUB_STEP_SUMMARY
                else
                  echo "- :x: \`${SCRIPT_ID}\`: ${PASS_RATE}% pass rate" >> $GITHUB_STEP_SUMMARY
                fi
              fi
            done
          else
            echo "No proofs generated." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID:** \`${{ github.run_id }}\`" >> $GITHUB_STEP_SUMMARY

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            test-results/junit.xml
            test-results/test-metrics.json
            test-results/pytest-output.log
          retention-days: 30

      - name: Upload Certification Proofs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: certification-proofs
          path: test-results/proofs/
          retention-days: 90

      - name: Fail if tests failed
        if: steps.tests.outputs.exit_code != '0'
        run: |
          echo " Tests failed with exit code ${{ steps.tests.outputs.exit_code }}"
          exit 1

  record-test-metrics:
    name: Record Test Metrics
    runs-on: ubuntu-latest
    needs: [unit-tests]
    if: github.event_name == 'push'
    permissions:
      contents: write

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download Test Metrics
        uses: actions/download-artifact@v4
        with:
          name: test-results
          path: test-results

      - name: Record Metrics to Governance Registry
        env:
          GOVERNANCE_REGISTRY_BRANCH: governance-registry
        run: |
          if [ ! -f test-results/test-metrics.json ]; then
            echo "No test-metrics.json found; skipping registry write"
            exit 0
          fi

          ENV_NAME="dev"
          if [ "${{ github.ref_name }}" = "main" ]; then
            ENV_NAME="prod"
          fi

          bash scripts/record-test-metrics.sh "$ENV_NAME" test-results/test-metrics.json

  verify-proofs:
    name: Verify Script Certification
    runs-on: ubuntu-latest
    needs: [unit-tests]
    if: always() && needs.unit-tests.result == 'success'

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download Proofs
        uses: actions/download-artifact@v4
        with:
          name: certification-proofs
          path: test-results/proofs

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: pip install pyyaml

      - name: Verify Script Certification
        run: |
          echo " Verifying scripts with proof artifacts..."

          # Run validator with proof verification
          python3 scripts/validate_scripts_tested.py --verify-proofs

      - name: Certification Summary
        if: always()
        run: |
          echo "## Script Certification Verification" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -d test-results/proofs ]; then
            TOTAL=$(ls -1 test-results/proofs/*.json 2>/dev/null | wc -l)
            PASSED=$(grep -l '"verdict": "PASS"' test-results/proofs/*.json 2>/dev/null | wc -l)
            FAILED=$((TOTAL - PASSED))

            echo "| Status | Count |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Scripts with proofs | ${TOTAL} |" >> $GITHUB_STEP_SUMMARY
            echo "| Certified (PASS) | ${PASSED} |" >> $GITHUB_STEP_SUMMARY
            echo "| Not certified (FAIL) | ${FAILED} |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Reference: [ADR-0146 Schema-Driven Script Certification](docs/adrs/ADR-0146-schema-driven-script-certification.md)" >> $GITHUB_STEP_SUMMARY
